{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from tensorflow.contrib.tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./dataset/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./dataset/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./dataset/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Iter 500, Testing Accuracy: 0.919200,Train Accuracy:0.915091\n",
      "Iter 1000, Testing Accuracy: 0.951500,Train Accuracy:0.950745\n",
      "Iter 1500, Testing Accuracy: 0.963100,Train Accuracy:0.965818\n",
      "Iter 2000, Testing Accuracy: 0.970500,Train Accuracy:0.974400\n",
      "Iter 2500, Testing Accuracy: 0.973400,Train Accuracy:0.979109\n",
      "Iter 3000, Testing Accuracy: 0.975100,Train Accuracy:0.982873\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "mnist = input_data.read_data_sets('./dataset/MNIST_data',one_hot=True)\n",
    "\n",
    "# param set\n",
    "batch_size = 2000                                  #单批次数据量\n",
    "n_batch = mnist.train.num_examples // batch_size  #批次数量\n",
    "hidden_layer1_node = 500                         #隐藏层1节点数\n",
    "hidden_layer2_node = 300                         #隐藏层2节点数\n",
    "output_layer_node = 10                            #输出层节点数\n",
    "steps = 3000                                        #训练迭代次数\n",
    "lr = 0.3                                          #学习速率\n",
    "regularization_rate = 0.0015                       #正则化系数\n",
    "image_num = 300\n",
    "\n",
    "embedding = tf.Variable(tf.stack(mnist.test.images[:image_num]),trainable=False,name='embedding')\n",
    "\n",
    "# 训练数据插入点\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32,[None,784])\n",
    "    y = tf.placeholder(tf.float32,[None,output_layer_node])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# display image\n",
    "\n",
    "image_shaped_input = tf.reshape(x,[-1,28,28,1])\n",
    "tf.summary.image('input',image_shaped_input,10)\n",
    "    \n",
    "def variable_summaries(var, name):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)  \n",
    "        tf.summary.scalar('mean/' + name, mean)  \n",
    "        with tf.name_scope('stddev'):  \n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))  \n",
    "        tf.summary.scalar('sttdev/' + name, stddev)  \n",
    "        tf.summary.scalar('max/' + name, tf.reduce_max(var))  \n",
    "        tf.summary.scalar('min/' + name, tf.reduce_min(var))  \n",
    "        tf.summary.histogram(name, var) \n",
    "        \n",
    "def inference(input_tensor,hidden_layer1_node,hidden_layer2_node,\n",
    "              keep_prob,regularization_rate,reuse=False):\n",
    "# weight & biases\n",
    "    with tf.variable_scope('hidden_layer1',reuse=reuse):\n",
    "        hidden_layer1_w = tf.get_variable('w',[784,hidden_layer1_node],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        variable_summaries(hidden_layer1_w, 'hidden_layer1_w')\n",
    "        hidden_layer1_b = tf.get_variable('b',hidden_layer1_node,\n",
    "                                          initializer=tf.constant_initializer(0.1))\n",
    "        variable_summaries(hidden_layer1_b, 'hidden_layer1_b')\n",
    "        hidden_layer1 = tf.matmul(x,hidden_layer1_w) + hidden_layer1_b\n",
    "        hidden_layer1 = tf.nn.relu(hidden_layer1)\n",
    "        hidden_layer1 = tf.nn.dropout(x=hidden_layer1,keep_prob=keep_prob)\n",
    "        \n",
    "    with tf.variable_scope('hidden_layer2',reuse=reuse):\n",
    "        hidden_layer2_w = tf.get_variable('w',[hidden_layer1_node,hidden_layer2_node],\n",
    "                                           initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        variable_summaries(hidden_layer2_w, 'hidden_layer2_w')\n",
    "        hidden_layer2_b = tf.get_variable('b',hidden_layer2_node,\n",
    "                                           initializer=tf.constant_initializer(0.1))\n",
    "        variable_summaries(hidden_layer2_b, 'hidden_layer2_b')\n",
    "        hidden_layer2 = tf.matmul(hidden_layer1,hidden_layer2_w) + hidden_layer2_b\n",
    "        hidden_layer2 = tf.nn.relu(hidden_layer2)\n",
    "        hidden_layer2 = tf.nn.dropout(x=hidden_layer2,keep_prob=keep_prob)\n",
    "        \n",
    "    with tf.variable_scope('out_layer',reuse=reuse):\n",
    "        out_layer_w = tf.get_variable('w',[hidden_layer2_node,output_layer_node],\n",
    "                                      initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "        variable_summaries(out_layer_w, 'out_layer_w')\n",
    "        out_layer_b = tf.get_variable('b',output_layer_node,\n",
    "                                      initializer=tf.constant_initializer(0.1))\n",
    "        variable_summaries(out_layer_b, 'out_layer_b')\n",
    "        prediction = tf.matmul(hidden_layer2,out_layer_w) + out_layer_b\n",
    "        \n",
    "    with tf.variable_scope('regular',reuse=reuse):\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(regularization_rate)\n",
    "        regularization = regularizer(hidden_layer1_w) + regularizer(hidden_layer2_w) + regularizer(out_layer_w)\n",
    "        variable_summaries(regularization, 'regularization')\n",
    "    return prediction,regularization\n",
    "\n",
    "prediction,regularization = inference(x,hidden_layer1_node,hidden_layer2_node,keep_prob,regularization_rate) \n",
    "\n",
    "# 损失函数\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction)) + regularization\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "# 定义训练方式\n",
    "with tf.name_scope('train_step'):\n",
    "    train_step = tf.train.AdadeltaOptimizer(lr).minimize(loss)\n",
    "\n",
    "#初始化变量\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "#准确率\n",
    "with tf.name_scope('accuracy'):\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# runtime\n",
    "with tf.Session() as sess:\n",
    "    #初始化变量\n",
    "    sess.run(init)\n",
    "    \n",
    "    #write metadata\n",
    "    if tf.gfile.Exists('./tensorborad/logs/metadata.tsv'):\n",
    "        tf.gfile.DeleteRecursively('./tensorborad/logs/metadata.tsv')\n",
    "    with open('./tensorborad/logs/metadata.tsv','w') as f:\n",
    "        labels = sess.run(tf.argmax(mnist.test.labels[:],1))\n",
    "        for i in range(image_num):\n",
    "            f.write(str(labels[i]) + '\\n')\n",
    "\n",
    "    #merged summary\n",
    "    merged = tf.summary.merge_all() \n",
    "    \n",
    "    writer = tf.summary.FileWriter('./tensorborad/logs',tf.get_default_graph())\n",
    "    saver = tf.train.Saver()\n",
    "    config = projector.ProjectorConfig()\n",
    "    embed = config.embeddings.add()\n",
    "    embed.tensor_name = embedding.name\n",
    "    embed.metadata_path = './tensorborad/logs/metadata.tsv'\n",
    "    embed.sprite.image_path = './tensorborad/logs/mnist_10k_sprite.png'\n",
    "    embed.sprite.single_image_dim.extend([28,28])\n",
    "    projector.visualize_embeddings(writer,config)\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter('./tensorborad/logs/train')\n",
    "    test_writer = tf.summary.FileWriter('./tensorborad/logs/test')\n",
    "    for i in range(steps):\n",
    "        x_train,y_train = mnist.train.next_batch(i)\n",
    "        sess.run(train_step,feed_dict={x:x_train,y:y_train,keep_prob:0.8})\n",
    "        if (i+1) % 500 ==0:\n",
    "            summary, acc = sess.run([merged,accuracy],feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "            train_writer.add_summary(summary, i) \n",
    "            summary,train_acc = sess.run([merged,accuracy],feed_dict={x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0})\n",
    "            test_writer.add_summary(summary, i) \n",
    "            print('Iter %d, Testing Accuracy: %f,Train Accuracy:%f' % (i+1,acc,train_acc))\n",
    "            saver.save(sess,'./tensorborad/logs/model.ckpt',i)\n",
    "    \n",
    "train_writer.close()\n",
    "test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, rand, tpe, space_eval, partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_layer1_node = 500                         #隐藏层1节点数\n",
    "hidden_layer2_node = 300                         #隐藏层2节点数\n",
    "lr = 0.3                                          #学习速率\n",
    "regularization_rate = 0.0015                       #正则化系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space = [hp.randint('hidden_layer1_node',10,1000),\n",
    "         hp.randint('hidden_layer2_node',10,1000),\n",
    "         hp.normal('lr',0,1),\n",
    "         hp.normal('regularization_rate',0,0.01),\n",
    "         hp.normal('keep_prob',0,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<hyperopt.pyll.base.Apply at 0x10eab0da0>,\n",
       " <hyperopt.pyll.base.Apply at 0x10eab0160>,\n",
       " <hyperopt.pyll.base.Apply at 0x10eab03c8>,\n",
       " <hyperopt.pyll.base.Apply at 0x10eab0128>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-2577f7411c47>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-2577f7411c47>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print sample(space)\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print sample(space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percept(args):\n",
    "    global X_train_std,y_train,y_test\n",
    "    ppn,regularization = inference(input_tensor=x,hidden_layer1_node = int(args['hidden_layer1_node']),\n",
    "                                   hidden_layer2_node= int(args[hidden_layer2_node]),\n",
    "                                   keep_prob = args['keep_prob'],\n",
    "                                   regularization_rate = args['regularization_rate'],\n",
    "                                   reuse=False)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=ppn)) + regularization\n",
    "    train_step = tf.train.AdadeltaOptimizer(args['lr']).minimize(loss)\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    return -accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ap_categorical_sampler() got multiple values for argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d43f624040e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_startup_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercept\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/rockchen/anaconda/envs/tensorflow/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     verbose=verbose)\n\u001b[1;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rockchen/anaconda/envs/tensorflow/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rockchen/anaconda/envs/tensorflow/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    155\u001b[0m                                                   d['result'].get('status')))\n\u001b[1;32m    156\u001b[0m                 new_trials = algo(new_ids, self.domain, trials,\n\u001b[0;32m--> 157\u001b[0;31m                                   self.rstate.randint(2 ** 31 - 1))\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rockchen/anaconda/envs/tensorflow/lib/python3.5/site-packages/hyperopt/tpe.py\u001b[0m in \u001b[0;36msuggest\u001b[0;34m(new_ids, domain, trials, seed, prior_weight, n_startup_jobs, n_EI_candidates, gamma, linear_forgetting)\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0ms_prior_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobserved_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0;34m=\u001b[0m \u001b[0mtpe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m     \u001b[0mtt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tpe_transform took %f seconds'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rockchen/anaconda/envs/tensorflow/lib/python3.5/site-packages/hyperopt/tpe.py\u001b[0m in \u001b[0;36mtpe_transform\u001b[0;34m(domain, prior_weight, gamma)\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mobserved_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vals'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m         \u001b[0mpyll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m         \u001b[0ms_prior_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m     )\n\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rockchen/anaconda/envs/tensorflow/lib/python3.5/site-packages/hyperopt/tpe.py\u001b[0m in \u001b[0;36mbuild_posterior\u001b[0;34m(specs, prior_idxs, prior_vals, obs_idxs, obs_vals, oloss_idxs, oloss_vals, oloss_gamma, prior_weight)\u001b[0m\n\u001b[1;32m    682\u001b[0m                 named_args = [[kw, memo[arg]]\n\u001b[1;32m    683\u001b[0m                               for (kw, arg) in node.named_args]\n\u001b[0;32m--> 684\u001b[0;31m                 \u001b[0mb_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mb_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamed_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m                 \u001b[0ma_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mobs_above\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_weight\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0ma_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamed_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ap_categorical_sampler() got multiple values for argument 'size'"
     ]
    }
   ],
   "source": [
    "def NN(argsDict):\n",
    "    hidden_layer1_node = argsDict['hidden_layer1_node']                         #隐藏层1节点数\n",
    "    hidden_layer2_node = argsDict['hidden_layer2_node']                         #隐藏层2节点数\n",
    "    lr = argsDict['lr']                                                         #学习速率\n",
    "    regularization_rate = argsDict['regularization_rate']                       #正则化系数\n",
    "    keep_prob = argsDict['keep_prob']                                           #dropout系数\n",
    "    print 'hidden_layer1_node:' + str(hidden_layer1_node)\n",
    "    print 'hidden_layer2_node:' + str(hidden_layer2_node)\n",
    "    print 'lr' + str(lr)\n",
    "    print 'regularization_rate' + str(regularization_rate)\n",
    "    print 'keep_prob' + str(keep_prob)\n",
    "    global attr_train,label_train\n",
    "    \n",
    "    nn = train_nn(hidden_layer1_node,hidden_layer2_node,keep_prob,regularization_rate)\n",
    "    loss ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000                                  #单批次数据量\n",
    "n_batch = mnist.train.num_examples // batch_size  #批次数量\n",
    "hidden_layer1_node = 500                         #隐藏层1节点数\n",
    "hidden_layer2_node = 300                         #隐藏层2节点数\n",
    "output_layer_node = 10                            #输出层节点数\n",
    "steps = 3000                                        #训练迭代次数\n",
    "lr = 0.3                                          #学习速率\n",
    "regularization_rate = 0.0015                       #正则化系数\n",
    "image_num = 10000\n",
    "def train_nn(hidden_layer1_node,hidden_layer2_node,keep_prob,regularization_rate):\n",
    "    def inference(input_tensor,hidden_layer1_node,hidden_layer2_node,\n",
    "                  keep_prob,regularization_rate,reuse=False):\n",
    "    # weight & biases\n",
    "        with tf.variable_scope('hidden_layer1',reuse=reuse):\n",
    "            hidden_layer1_w = tf.get_variable('w',[784,hidden_layer1_node],initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            variable_summaries(hidden_layer1_w, 'hidden_layer1_w')\n",
    "            hidden_layer1_b = tf.get_variable('b',hidden_layer1_node,\n",
    "                                              initializer=tf.constant_initializer(0.1))\n",
    "            variable_summaries(hidden_layer1_b, 'hidden_layer1_b')\n",
    "            hidden_layer1 = tf.nn.relu(tf.matmul(x,hidden_layer1_w) + hidden_layer1_b)\n",
    "            hidden_layer1 = tf.nn.dropout(x=hidden_layer1,keep_prob=keep_prob)\n",
    "\n",
    "        with tf.variable_scope('hidden_layer2',reuse=reuse):\n",
    "            hidden_layer2_w = tf.get_variable('w',[hidden_layer1_node,hidden_layer2_node],\n",
    "                                               initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            variable_summaries(hidden_layer2_w, 'hidden_layer2_w')\n",
    "            hidden_layer2_b = tf.get_variable('b',hidden_layer2_node,\n",
    "                                               initializer=tf.constant_initializer(0.1))\n",
    "            variable_summaries(hidden_layer2_b, 'hidden_layer2_b')\n",
    "            hidden_layer2 = tf.nn.relu(tf.matmul(hidden_layer1,hidden_layer2_w) + hidden_layer2_b)\n",
    "            hidden_layer2 = tf.nn.dropout(x=hidden_layer2,keep_prob=keep_prob)\n",
    "\n",
    "        with tf.variable_scope('out_layer',reuse=reuse):\n",
    "            out_layer_w = tf.get_variable('w',[hidden_layer2_node,output_layer_node],\n",
    "                                          initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "            variable_summaries(out_layer_w, 'out_layer_w')\n",
    "            out_layer_b = tf.get_variable('b',output_layer_node,\n",
    "                                          initializer=tf.constant_initializer(0.1))\n",
    "            variable_summaries(out_layer_b, 'out_layer_b')\n",
    "            prediction = tf.matmul(hidden_layer2,out_layer_w) + out_layer_b\n",
    "        with tf.variable_scope('regular',reuse=reuse):\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(regularization_rate)\n",
    "            regularization = regularizer(hidden_layer1_w) + regularizer(hidden_layer2_w) + regularizer(out_layer_w)\n",
    "            variable_summaries(regularization, 'regularization')\n",
    "        return prediction,regularization\n",
    "\n",
    "    prediction,regularization = inference(x,keep_prob,regularization_rate) \n",
    "\n",
    "    # 损失函数\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits=prediction)) + regularization\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # 定义训练方式\n",
    "    with tf.name_scope('train_step'):\n",
    "        train_step = tf.train.AdadeltaOptimizer(lr).minimize(loss)\n",
    "\n",
    "    #初始化变量\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    #准确率\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(prediction,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # runtime\n",
    "    with tf.Session() as sess:\n",
    "        #初始化变量\n",
    "        sess.run(init)\n",
    "\n",
    "        #write metadata\n",
    "        if tf.gfile.Exists('./tensorborad/logs/metadata.tsv'):\n",
    "            tf.gfile.DeleteRecursively('./tensorborad/logs/metadata.tsv')\n",
    "        with open('./tensorborad/logs/metadata.tsv','w') as f:\n",
    "            labels = sess.run(tf.argmax(mnist.test.labels[:],1))\n",
    "            for i in range(image_num):\n",
    "                f.write(str(labels[i]) + '\\n')\n",
    "\n",
    "        #merged summary\n",
    "        merged = tf.summary.merge_all() \n",
    "\n",
    "        writer = tf.summary.FileWriter('./tensorborad/logs',tf.get_default_graph())\n",
    "        saver = tf.train.Saver()\n",
    "        config = projector.ProjectorConfig()\n",
    "        embed = config.embeddings.add()\n",
    "        embed.tensor_name = embedding.name\n",
    "        #embed.metadata_path = './tensorborad/logs/metadata.tsv'\n",
    "        #embed.sprite.image_path = './tensorborad/logs/mnist_10k_sprite.png'\n",
    "        embed.sprite.single_image_dim.extend([28,28])\n",
    "        projector.visualize_embeddings(writer,config)\n",
    "\n",
    "        train_writer = tf.summary.FileWriter('./tensorborad/logs/train')\n",
    "        test_writer = tf.summary.FileWriter('./tensorborad/logs/test')\n",
    "        for i in range(steps):\n",
    "            x_train,y_train = mnist.train.next_batch(i)\n",
    "            sess.run(train_step,feed_dict={x:x_train,y:y_train,keep_prob:0.8})\n",
    "            if (i+1) % 50 ==0:\n",
    "                summary, acc = sess.run([merged,accuracy],feed_dict={x:mnist.test.images,y:mnist.test.labels,keep_prob:1.0})\n",
    "                train_writer.add_summary(summary, i) \n",
    "                summary,train_acc = sess.run([merged,accuracy],feed_dict={x:mnist.train.images,y:mnist.train.labels,keep_prob:1.0})\n",
    "                test_writer.add_summary(summary, i) \n",
    "                print('Iter %d, Testing Accuracy: %f,Train Accuracy:%f' % (i+1,acc,train_acc))\n",
    "                saver.save(sess,'./tensorborad/logs/model.ckpt',i)\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
